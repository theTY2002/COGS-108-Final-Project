{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project (change this to your project's title)\n",
    "\n",
    "# Permissions\n",
    "\n",
    "Place an `X` in the appropriate bracket below to specify if you would like your group's project to be made available to the public. (Note that student names will be included (but PIDs will be scraped from any groups who include their PIDs).\n",
    "\n",
    "* [  ] YES - make available\n",
    "* [  ] NO - keep private\n",
    "\n",
    "# Names\n",
    "\n",
    "- Kavin Raj\n",
    "- Arnav Saxena\n",
    "- Tiantong Wu\n",
    "- Peike Xu\n",
    "- Jing Yin Yip\n",
    "\n",
    "# Abstract\n",
    "\n",
    "Please write one to four paragraphs that describe a very brief overview of why you did this, how you did, and the major findings and conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Include a specific, clear data science question.\n",
    "-  Make sure what you're measuring (variables) to answer the question is clear\n",
    "\n",
    "What is your research question? Include the specific question you're setting out to answer. This question should be specific, answerable with data, and clear. A general question with specific subquestions is permitted. (1-2 sentences)\n",
    "\n",
    "How do opinionated tweets on Twitter as measured by a sentiment analysis model affect the stock prices of major tech companies, such as Apple, Amazon, Google, and Tesla within the time period of **(insert new time period here)**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Include a general introduction to your topic\n",
    "- Include explanation of what work has been done previously\n",
    "- Include citations or links to previous work\n",
    "\n",
    "This section will present the background and context of your topic and question in a few paragraphs. Include a general introduction to your topic and then describe what information you currently know about the topic after doing your initial research. Include references to other projects who have asked similar questions or approached similar problems. Explain what others have learned in their projects.\n",
    "\n",
    "Find some relevant prior work, and reference those sources, summarizing what each did and what they learned. Even if you think you have a totally novel question, find the most similar prior work that you can and discuss how it relates to your project.\n",
    "\n",
    "References can be research publications, but they need not be. Blogs, GitHub repositories, company websites, etc., are all viable references if they are relevant to your project. It must be clear which information comes from which references. (2-3 paragraphs, including at least 2 references)\n",
    "\n",
    "**(Below is what we wrote before, but the group that created the new dataset also has another paper that could be incorporated into here: https://www.kaggle.com/datasets/omermetinn/tweets-about-the-top-companies-from-2015-to-2020/versions/1?resource=download)**\n",
    "\n",
    "Twitter is a widely used social media platform for expressing many kinds of opinions, including those on certain companies. We believe that this may have an impact on the public perception of these companies, and we aim to investigate whether there is a connection between opinions expressed in social media and the actual stock prices of the companies in question. An example of was seen this back in 2021, when “a thread on r/WallStreetBets”  caused “more than 7,200% increase in GME—and a 689% run”<a name=\"cite_note-1\"></a> <sup>[<a href=\"#cite_ref-1\">1</a>]</sup> . This occurence informed us that there is a potential causal effect between opinions on social media and real-world stock prices; we think that this effect has a much greater scope than just this isolated case of GameStop stocks, and we are interested in seeing if this is a larger, more general phenomenon that can be applied to other time periods and companies.\n",
    "\n",
    "When reading a research paper from the IOP conference series, we found a sentiment analysis model based on social media opinion on stock trading. It was remarked in the conclusion that “looking into correlation coefficient compared by number of days before and after the trading day, the result shows that correlation reaches to the peak on trading day then it gradually declines with the magnitude depending on the day length after trading day.” <a name=\"cite_note-2\"></a> <sup>[<a href=\"#cite_ref-2\">2</a>]</sup> This research is similar to what we aim to investigate, as the paper conducted their test on a Thai social media platform called Pantip, and discussed ten Thai companies. The paper provides more evidence that there is a causal link between social media sentiment and stock prices, and we are interested to see if a similar trend can be seen with tweets and tech companies' stocks in the US. Additionally, we would also like to investigate if there are other similarities or differences in the trends that we are able to identify between our research and the paper, such as the correlation between variables reaching a peak on trading day.\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> <sup>[<a href=\"#cite_ref-1\">1</a>]</sup> Rechel, J. (28 Jan 2021) How social media moves markets: Analyzing GameStop (GME) using social listening data. Sprout Blog.\n",
    "   <br> https://sproutsocial.com/insights/gamestop-stock-social-media\n",
    "\n",
    "2. <a name=\"cite_note-2\"></a> <sup>[<a href=\"#cite_ref-2\">2</a>]</sup>  P Padhanarath et al 2019 IOP Conf. Ser.: Mater. Sci. Eng. 620 012094.\n",
    "   <br>https://iopscience.iop.org/article/10.1088/1757-899X/620/1/012094/pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before answering our research question by investigating the available data, we think that there will be a positive correlation between a positive sentiment for a company and said company’s stock prices rising, as well as a positive correlation between a negative sentiment for a company and said company’s stock prices falling. This is due to the connection between a company’s public perception, how that is reflected in social media, and how it manifests in the stock market. If tweets about a company are mostly negative within a certain time period, we would expect to observe a decrease in stock prices, as both phenomena correspond to a decrease in public perception of the company. However, we also acknowledge that this relationship may not be as straightforward as is stated here, as there may be other confounds affecting each variable, such as Twitter only capturing the sentiment of a more vocal sample of people as compared to the rest of the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "Tweet Data:\n",
    "- Dataset #1\n",
    "  - Dataset Name: Stock prediction based on Tweet Sentiment Analysis\n",
    "  - Link to the dataset: https://www.kaggle.com/code/shreytandel19/stock-prediction-based-on-tweet-sentiment-analysis/input\n",
    "  - Number of observations: 80793\n",
    "  - Number of variables: 4\n",
    "- Dataset #2\n",
    "  - Dataset Name: Tweets about the Top Companies from 2015 to 2020\n",
    "  - Link to the dataset: https://www.kaggle.com/datasets/omermetinn/tweets-about-the-top-companies-from-2015-to-2020?select=Tweet.csv\n",
    "  - Number of observations: 3717964\n",
    "  - Number of variables: 7\n",
    "\n",
    "Stock Data:\n",
    "- Dataset #3\n",
    "  - Dataset Name: TESLA_HISTORICAL\n",
    "  - Link to the dataset: https://www.nasdaq.com/market-activity/stocks/tsla/historical\n",
    "  - Number of observations: 1258\n",
    "  - Number of variables: 6\n",
    "- Dataset #4\n",
    "  - Dataset Name: APPLE_HISTORICAL\n",
    "  - Link to the dataset: https://www.nasdaq.com/market-activity/stocks/aapl/historical\n",
    "  - Number of observations: 1258\n",
    "  - Number of variables: 6\n",
    "- Dataset #5\n",
    "  - Dataset Name: AMAZON_HISTORICAL\n",
    "  - Link to the dataset: https://www.nasdaq.com/market-activity/stocks/amzn/historical\n",
    "  - Number of observations: 1258\n",
    "  - Number of variables: 6\n",
    "- Dataset #6\n",
    "  - Dataset Name: GOOGL_HISTORICAL\n",
    "  - Link to the dataset: https://www.nasdaq.com/market-activity/stocks/googl/historical\n",
    "  - Number of observations: 1258\n",
    "  - Number of variables: 6\n",
    "- Dataset #7\n",
    "  - Dataset Name: GOOG_HISTORICAL\n",
    "  - Link to the dataset: https://www.nasdaq.com/market-activity/stocks/goog/historical\n",
    "  - Number of observations: 1258\n",
    "  - Number of variables: 6\n",
    "- Dataset #8\n",
    "  - Dataset Name: MICROSOFT_HISTORICAL\n",
    "  - Link to the dataset: https://www.nasdaq.com/market-activity/stocks/msft/historical\n",
    "  - Number of observations: 1258\n",
    "  - Number of variables: 6\n",
    "\n",
    "\n",
    "The first dataset, Tweet Stock, is a collection of tweets that talk about TSLA, the stock for the company Tesla. The time range for this datasets is from September 30th, 2021 to September 29th, 2022, and was chosen  since we are unable to obtain current Twitter data, and need to rely on past data that has already been collected. The two important variables in this dataset are the dates that the tweets were posted, as well as the tweets themselves. The tweets are in the form of strings, and the dates will be converted into a standard Timestamp format. The tweets in this dataset will be used as a proxy for social media opinion, once it is processed by a sentiment analysis model.\n",
    "\n",
    "The second dataset, TESLA_HISTORICAL, is a record of prices of the TSLA stock on each day from September 30th, 2021 to September 29th, 2022. This time range was chosen to match the time range of the other dataset. In the dataset, the highest, lowest, and open/close prices of the stock are recorded as floats in each row, along with the corresponding day, which will be converted into a standard Timestamp format. These variables can provide insight into how a stock varies within a day, and comparing these across different days can lead to potential trends over time. The prices are a direct factual record of TSLA prices, although further numerical processing may be needed later to reveal meaningful trends.\n",
    "\n",
    "Further details of the specific steps taken to clean the datasets will be explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\saxen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saxen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saxen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "import datetime \n",
    "from datetime import timedelta \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset #1 Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset, containing tweets for all companies.\n",
    "tweets_multiple = pd.read_csv('data/Tweet.csv')\n",
    "id_to_company = pd.read_csv('data/Company_Tweet.csv')\n",
    "tweets_multiple = tweets_multiple.join(id_to_company.set_index('tweet_id'), on='tweet_id')\n",
    "#delete any missing value within the dataset\n",
    "tweets_multiple.dropna(inplace=True)\n",
    "\n",
    "tweets_multiple = tweets_multiple[['post_date', 'body', 'ticker_symbol']]\n",
    "#drop irrelvant columns\n",
    "\n",
    "tweets_multiple['Date'] = pd.to_datetime(tweets_multiple['post_date'], unit='s')\n",
    "\n",
    "tweets_multiple = tweets_multiple[['Date', 'body', 'ticker_symbol']]\n",
    "tweets_multiple.columns = ['Date', 'Tweet', 'Stock Name']\n",
    "tweets_multiple = tweets_multiple.sort_values(by=['Stock Name', 'Date'])\n",
    "\n",
    "tweets_multiple = tweets_multiple.reset_index(drop=True)\n",
    "tweets_multiple[\"isStatusChanged\"] = tweets_multiple[\"Stock Name\"].shift(1, fill_value=tweets_multiple[\"Stock Name\"].head(1)) != tweets_multiple[\"Stock Name\"]\n",
    "\n",
    "tweets_multiple.loc[tweets_multiple[\"isStatusChanged\"] == True]\n",
    "\n",
    "tweets_multiple['month'] = tweets_multiple['Date'].dt.to_period('M').astype(str)\n",
    "\n",
    "apple_tweets = tweets_multiple.iloc[:1414802].drop(columns=['isStatusChanged']).reset_index(drop=True)\n",
    "amazon_tweets = tweets_multiple.iloc[1414802:2123994].drop(columns=['isStatusChanged']).reset_index(drop=True)\n",
    "google_tweets = tweets_multiple.iloc[2123994:2833526].drop(columns=['isStatusChanged']).reset_index(drop=True)\n",
    "microsoft_tweets = tweets_multiple.iloc[2833526:3207324].drop(columns=['isStatusChanged']).reset_index(drop=True)\n",
    "tesla_tweets = tweets_multiple.iloc[3207324:].drop(columns=['isStatusChanged']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = SentimentIntensityAnalyzer()\n",
    "\n",
    "datasets = [apple_tweets, amazon_tweets, google_tweets, microsoft_tweets, tesla_tweets]\n",
    "\n",
    "for df in datasets:\n",
    "    #sentiment analysis\n",
    "    df['sentimental score'] = df['Tweet'].apply(sentiment.polarity_scores)\n",
    "\n",
    "    compound_score = []\n",
    "    for score in df['sentimental score']:\n",
    "        compound_score.append(score['compound'])\n",
    "    compound_score\n",
    "\n",
    "    df['sentimental score'] = compound_score\n",
    "\n",
    "apple_tweets = datasets[0]\n",
    "amazon_tweets = datasets[1]\n",
    "google_tweets = datasets[2]\n",
    "microsoft_tweets = datasets[3]\n",
    "tesla_tweets = datasets[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to file to avoid reading from large csv every time\n",
    "apple_tweets.to_csv('data/Apple_Tweets.csv', index=False)\n",
    "amazon_tweets.to_csv('data/Amazon_Tweets.csv', index=False)\n",
    "google_tweets.to_csv('data/Google_Tweets.csv', index=False)\n",
    "microsoft_tweets.to_csv('data/Microsoft_Tweets.csv', index=False)\n",
    "tesla_tweets.to_csv('data/Tesla_Tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stock Name</th>\n",
       "      <th>month</th>\n",
       "      <th>sentimental score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:01:50</td>\n",
       "      <td>S&amp;P100 #Stocks Performance $HD $LOW $SBUX $TGT...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2015-01</td>\n",
       "      <td>-0.4278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 00:26:09</td>\n",
       "      <td>Top 10 searched #stocks of #2014 $AAPL $FB $BA...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2015-01</td>\n",
       "      <td>0.2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 00:47:17</td>\n",
       "      <td>RT @SeekingAlpha: A Look At BlackBerry's Deals...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2015-01</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 00:49:05</td>\n",
       "      <td>Jeff Bezos lost $7.4 billion in Amazon's worst...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2015-01</td>\n",
       "      <td>-0.7506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 00:50:10</td>\n",
       "      <td>Jeff Bezos lost $7.4 billion in #Amazon worst ...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2015-01</td>\n",
       "      <td>-0.6908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709187</th>\n",
       "      <td>2019-12-31 23:29:26</td>\n",
       "      <td>$AMZN Amazon Stock Broke Out -- and Hesitated;...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2019-12</td>\n",
       "      <td>-0.5267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709188</th>\n",
       "      <td>2019-12-31 23:31:28</td>\n",
       "      <td>[Yahoo Finance Video - December 31, 2019] $AMZ...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2019-12</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709189</th>\n",
       "      <td>2019-12-31 23:35:01</td>\n",
       "      <td>@Ryzenn $AAPL has to have a lights out ER to k...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2019-12</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709190</th>\n",
       "      <td>2019-12-31 23:41:10</td>\n",
       "      <td>Is there a public database or website where I ...</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2019-12</td>\n",
       "      <td>0.6908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709191</th>\n",
       "      <td>2019-12-31 23:43:26</td>\n",
       "      <td>$FB $AMZN $GOOGL $NFLX #FANG 2019</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>2019-12</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>709192 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Date                                              Tweet  \\\n",
       "0      2015-01-01 00:01:50  S&P100 #Stocks Performance $HD $LOW $SBUX $TGT...   \n",
       "1      2015-01-01 00:26:09  Top 10 searched #stocks of #2014 $AAPL $FB $BA...   \n",
       "2      2015-01-01 00:47:17  RT @SeekingAlpha: A Look At BlackBerry's Deals...   \n",
       "3      2015-01-01 00:49:05  Jeff Bezos lost $7.4 billion in Amazon's worst...   \n",
       "4      2015-01-01 00:50:10  Jeff Bezos lost $7.4 billion in #Amazon worst ...   \n",
       "...                    ...                                                ...   \n",
       "709187 2019-12-31 23:29:26  $AMZN Amazon Stock Broke Out -- and Hesitated;...   \n",
       "709188 2019-12-31 23:31:28  [Yahoo Finance Video - December 31, 2019] $AMZ...   \n",
       "709189 2019-12-31 23:35:01  @Ryzenn $AAPL has to have a lights out ER to k...   \n",
       "709190 2019-12-31 23:41:10  Is there a public database or website where I ...   \n",
       "709191 2019-12-31 23:43:26                  $FB $AMZN $GOOGL $NFLX #FANG 2019   \n",
       "\n",
       "       Stock Name    month  sentimental score  \n",
       "0            AMZN  2015-01            -0.4278  \n",
       "1            AMZN  2015-01             0.2023  \n",
       "2            AMZN  2015-01             0.0000  \n",
       "3            AMZN  2015-01            -0.7506  \n",
       "4            AMZN  2015-01            -0.6908  \n",
       "...           ...      ...                ...  \n",
       "709187       AMZN  2019-12            -0.5267  \n",
       "709188       AMZN  2019-12             0.3612  \n",
       "709189       AMZN  2019-12             0.3612  \n",
       "709190       AMZN  2019-12             0.6908  \n",
       "709191       AMZN  2019-12             0.0000  \n",
       "\n",
       "[709192 rows x 5 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "amazon_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset #2 Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesla_stock = pd.read_csv('data/TESLA_HISTORICAL.csv')\n",
    "apple_stock = pd.read_csv('data/APPLE_HISTORICAL.csv')\n",
    "amazon_stock = pd.read_csv('data/AMAZON_HISTORICAL.csv')\n",
    "microsoft_stock = pd.read_csv('data/MICROSOFT_HISTORICAL.csv')\n",
    "googl_stock = pd.read_csv('data/GOOGL_HISTORICAL.csv')\n",
    "goog_stock = pd.read_csv('data/GOOG_HISTORICAL.csv')\n",
    "\n",
    "stocks = [tesla_stock, apple_stock, amazon_stock, microsoft_stock, goog_stock, googl_stock]\n",
    "\n",
    "for i in range(len(stocks)):\n",
    "    stocks[i].dropna(inplace=True)\n",
    "    #delete any missing value within the dataset\n",
    "    stocks[i]['Date'] = pd.to_datetime(stocks[i].get('Date')).dt.date\n",
    "    #convert the date enteries into standarized form\n",
    "    def nodollartofloat(series):\n",
    "        series = series.str.strip('$')\n",
    "        blank = []\n",
    "        for i in series:\n",
    "            i = float(i)\n",
    "            blank = np.append(blank,i)\n",
    "        return blank\n",
    "    stocks[i]['End Day Price'] = nodollartofloat(stocks[i]['Close/Last'])\n",
    "    stocks[i]['Beginning Day Price'] = nodollartofloat(stocks[i]['Open'])\n",
    "    stocks[i]['Highest Price'] = nodollartofloat(stocks[i]['High'])\n",
    "    stocks[i]['Lowest Price'] = nodollartofloat(stocks[i]['Low'])\n",
    "    stocks[i] = stocks[i].drop(['Close/Last','Volume','Open','High','Low'], axis=1)\n",
    "    #convert all string input into number\n",
    "\n",
    "    start_date = pd.to_datetime('2015-01-01').date()\n",
    "    end_date = pd.to_datetime('2019-12-31').date()\n",
    "\n",
    "    stocks[i] = stocks[i][(stocks[i]['Date'] >= start_date) & (stocks[i]['Date'] <= end_date)]\n",
    "    stocks[i].reset_index(drop=True, inplace=True)\n",
    "    stocks[i][\"Raw Change in Price\"] = stocks[i][\"End Day Price\"] - stocks[i][\"Beginning Day Price\"]\n",
    "    stocks[i][\"% Change in Price\"] = (stocks[i][\"End Day Price\"] - stocks[i][\"Beginning Day Price\"])/stocks[i][\"Beginning Day Price\"]\n",
    "    stocks[i]['Date'] = pd.to_datetime(stocks[i]['Date'])\n",
    "    stocks[i]['month'] = stocks[i]['Date'].dt.to_period('M').astype(str)\n",
    "\n",
    "tesla_stock =  stocks[0]\n",
    "apple_stock = stocks[1]\n",
    "amazon_stock = stocks[2]\n",
    "microsoft_stock = stocks[3]\n",
    "goog_stock = stocks[4]\n",
    "googl_stock = stocks[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "Carry out whatever EDA you need to for your project.  Because every project will be different we can't really give you much of a template at this point. But please make sure you describe the what and why in text here as well as providing interpretation of results and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>End Day Price</th>\n",
       "      <th>Beginning Day Price</th>\n",
       "      <th>Highest Price</th>\n",
       "      <th>Lowest Price</th>\n",
       "      <th>Raw Change in Price</th>\n",
       "      <th>% Change in Price</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>27.8887</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>28.0860</td>\n",
       "      <td>26.8053</td>\n",
       "      <td>0.8887</td>\n",
       "      <td>0.032915</td>\n",
       "      <td>2019-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>27.6467</td>\n",
       "      <td>28.5860</td>\n",
       "      <td>28.6000</td>\n",
       "      <td>27.2839</td>\n",
       "      <td>-0.9393</td>\n",
       "      <td>-0.032859</td>\n",
       "      <td>2019-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>28.6920</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>29.0207</td>\n",
       "      <td>28.4073</td>\n",
       "      <td>-0.3080</td>\n",
       "      <td>-0.010621</td>\n",
       "      <td>2019-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>28.7293</td>\n",
       "      <td>28.5273</td>\n",
       "      <td>28.8987</td>\n",
       "      <td>28.4233</td>\n",
       "      <td>0.2020</td>\n",
       "      <td>0.007081</td>\n",
       "      <td>2019-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>28.3500</td>\n",
       "      <td>27.8907</td>\n",
       "      <td>28.3647</td>\n",
       "      <td>27.5125</td>\n",
       "      <td>0.4593</td>\n",
       "      <td>0.016468</td>\n",
       "      <td>2019-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>2015-01-08</td>\n",
       "      <td>14.0410</td>\n",
       "      <td>14.1873</td>\n",
       "      <td>14.2533</td>\n",
       "      <td>14.0007</td>\n",
       "      <td>-0.1463</td>\n",
       "      <td>-0.010312</td>\n",
       "      <td>2015-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>2015-01-07</td>\n",
       "      <td>14.0633</td>\n",
       "      <td>14.2233</td>\n",
       "      <td>14.3187</td>\n",
       "      <td>13.9853</td>\n",
       "      <td>-0.1600</td>\n",
       "      <td>-0.011249</td>\n",
       "      <td>2015-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>14.0853</td>\n",
       "      <td>14.0040</td>\n",
       "      <td>14.2800</td>\n",
       "      <td>13.6140</td>\n",
       "      <td>0.0813</td>\n",
       "      <td>0.005805</td>\n",
       "      <td>2015-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>14.0060</td>\n",
       "      <td>14.3033</td>\n",
       "      <td>14.4333</td>\n",
       "      <td>13.8108</td>\n",
       "      <td>-0.2973</td>\n",
       "      <td>-0.020785</td>\n",
       "      <td>2015-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>14.6207</td>\n",
       "      <td>14.8580</td>\n",
       "      <td>14.8833</td>\n",
       "      <td>14.2173</td>\n",
       "      <td>-0.2373</td>\n",
       "      <td>-0.015971</td>\n",
       "      <td>2015-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  End Day Price  Beginning Day Price  Highest Price  \\\n",
       "0    2019-12-31        27.8887              27.0000        28.0860   \n",
       "1    2019-12-30        27.6467              28.5860        28.6000   \n",
       "2    2019-12-27        28.6920              29.0000        29.0207   \n",
       "3    2019-12-26        28.7293              28.5273        28.8987   \n",
       "4    2019-12-24        28.3500              27.8907        28.3647   \n",
       "...         ...            ...                  ...            ...   \n",
       "1253 2015-01-08        14.0410              14.1873        14.2533   \n",
       "1254 2015-01-07        14.0633              14.2233        14.3187   \n",
       "1255 2015-01-06        14.0853              14.0040        14.2800   \n",
       "1256 2015-01-05        14.0060              14.3033        14.4333   \n",
       "1257 2015-01-02        14.6207              14.8580        14.8833   \n",
       "\n",
       "      Lowest Price  Raw Change in Price  % Change in Price    month  \n",
       "0          26.8053               0.8887           0.032915  2019-12  \n",
       "1          27.2839              -0.9393          -0.032859  2019-12  \n",
       "2          28.4073              -0.3080          -0.010621  2019-12  \n",
       "3          28.4233               0.2020           0.007081  2019-12  \n",
       "4          27.5125               0.4593           0.016468  2019-12  \n",
       "...            ...                  ...                ...      ...  \n",
       "1253       14.0007              -0.1463          -0.010312  2015-01  \n",
       "1254       13.9853              -0.1600          -0.011249  2015-01  \n",
       "1255       13.6140               0.0813           0.005805  2015-01  \n",
       "1256       13.8108              -0.2973          -0.020785  2015-01  \n",
       "1257       14.2173              -0.2373          -0.015971  2015-01  \n",
       "\n",
       "[1258 rows x 8 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tesla_stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Analysis You Did - Linear\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FROM HERE\n",
    "apple_tweets = pd.read_csv('data/Apple_Tweets.csv')\n",
    "amazon_tweets = pd.read_csv('data/Amazon_Tweets.csv')\n",
    "google_tweets = pd.read_csv('data/Google_Tweets.csv')\n",
    "microsoft_tweets = pd.read_csv('data/Microsoft_Tweets.csv')\n",
    "tesla_tweets = pd.read_csv('data/Tesla_Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_tweets_pos = apple_tweets[(apple_tweets['sentimental score'] > 0)].groupby(\"month\").mean().reset_index()\n",
    "apple_tweets_neg = apple_tweets[(apple_tweets['sentimental score'] < 0)].groupby(\"month\").mean().reset_index()\n",
    "\n",
    "amazon_tweets_pos = amazon_tweets[(amazon_tweets['sentimental score'] > 0)].groupby(\"month\").mean().reset_index()\n",
    "amazon_tweets_neg = amazon_tweets[(amazon_tweets['sentimental score'] < 0)].groupby(\"month\").mean().reset_index()\n",
    "\n",
    "google_tweets_pos = google_tweets[(google_tweets['sentimental score'] > 0)].groupby(\"month\").mean().reset_index()\n",
    "google_tweets_neg = google_tweets[(google_tweets['sentimental score'] < 0)].groupby(\"month\").mean().reset_index()\n",
    "\n",
    "microsoft_tweets_pos = microsoft_tweets[(microsoft_tweets['sentimental score'] > 0)].groupby(\"month\").mean().reset_index()\n",
    "microsoft_tweets_neg = microsoft_tweets[(microsoft_tweets['sentimental score'] < 0)].groupby(\"month\").mean().reset_index()\n",
    "\n",
    "tesla_tweets_pos = tesla_tweets[(tesla_tweets['sentimental score'] > 0)].groupby(\"month\").mean().reset_index()\n",
    "tesla_tweets_neg = tesla_tweets[(tesla_tweets['sentimental score'] < 0)].groupby(\"month\").mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>sentimental score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01</td>\n",
       "      <td>-0.379019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02</td>\n",
       "      <td>-0.346676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-03</td>\n",
       "      <td>-0.394009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04</td>\n",
       "      <td>-0.352792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-05</td>\n",
       "      <td>-0.326533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-06</td>\n",
       "      <td>-0.357311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-07</td>\n",
       "      <td>-0.369413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-08</td>\n",
       "      <td>-0.383557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-09</td>\n",
       "      <td>-0.346969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-10</td>\n",
       "      <td>-0.402662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2015-11</td>\n",
       "      <td>-0.354500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2015-12</td>\n",
       "      <td>-0.371351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-01</td>\n",
       "      <td>-0.389016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016-02</td>\n",
       "      <td>-0.376832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016-03</td>\n",
       "      <td>-0.421798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-04</td>\n",
       "      <td>-0.392485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016-05</td>\n",
       "      <td>-0.392585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016-06</td>\n",
       "      <td>-0.394081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016-07</td>\n",
       "      <td>-0.360814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016-08</td>\n",
       "      <td>-0.357164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016-09</td>\n",
       "      <td>-0.380766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2016-10</td>\n",
       "      <td>-0.368345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2016-11</td>\n",
       "      <td>-0.377120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2016-12</td>\n",
       "      <td>-0.422648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>-0.376765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017-02</td>\n",
       "      <td>-0.377449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2017-03</td>\n",
       "      <td>-0.374765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2017-04</td>\n",
       "      <td>-0.369288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2017-05</td>\n",
       "      <td>-0.377540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2017-06</td>\n",
       "      <td>-0.387702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2017-07</td>\n",
       "      <td>-0.375528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2017-08</td>\n",
       "      <td>-0.367253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2017-09</td>\n",
       "      <td>-0.360699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2017-10</td>\n",
       "      <td>-0.381396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2017-11</td>\n",
       "      <td>-0.392411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2017-12</td>\n",
       "      <td>-0.398965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-01</td>\n",
       "      <td>-0.386975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018-02</td>\n",
       "      <td>-0.392302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2018-03</td>\n",
       "      <td>-0.398778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2018-04</td>\n",
       "      <td>-0.378918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2018-05</td>\n",
       "      <td>-0.342527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2018-06</td>\n",
       "      <td>-0.347981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2018-07</td>\n",
       "      <td>-0.386048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2018-08</td>\n",
       "      <td>-0.346834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2018-09</td>\n",
       "      <td>-0.392426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018-10</td>\n",
       "      <td>-0.406775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2018-11</td>\n",
       "      <td>-0.409502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2018-12</td>\n",
       "      <td>-0.411870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2019-01</td>\n",
       "      <td>-0.375055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2019-02</td>\n",
       "      <td>-0.354317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2019-03</td>\n",
       "      <td>-0.363135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2019-04</td>\n",
       "      <td>-0.385145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2019-05</td>\n",
       "      <td>-0.416421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2019-06</td>\n",
       "      <td>-0.382959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2019-07</td>\n",
       "      <td>-0.387233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2019-08</td>\n",
       "      <td>-0.397097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2019-09</td>\n",
       "      <td>-0.399650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2019-10</td>\n",
       "      <td>-0.389866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2019-11</td>\n",
       "      <td>-0.377424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2019-12</td>\n",
       "      <td>-0.393139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      month  sentimental score\n",
       "0   2015-01          -0.379019\n",
       "1   2015-02          -0.346676\n",
       "2   2015-03          -0.394009\n",
       "3   2015-04          -0.352792\n",
       "4   2015-05          -0.326533\n",
       "5   2015-06          -0.357311\n",
       "6   2015-07          -0.369413\n",
       "7   2015-08          -0.383557\n",
       "8   2015-09          -0.346969\n",
       "9   2015-10          -0.402662\n",
       "10  2015-11          -0.354500\n",
       "11  2015-12          -0.371351\n",
       "12  2016-01          -0.389016\n",
       "13  2016-02          -0.376832\n",
       "14  2016-03          -0.421798\n",
       "15  2016-04          -0.392485\n",
       "16  2016-05          -0.392585\n",
       "17  2016-06          -0.394081\n",
       "18  2016-07          -0.360814\n",
       "19  2016-08          -0.357164\n",
       "20  2016-09          -0.380766\n",
       "21  2016-10          -0.368345\n",
       "22  2016-11          -0.377120\n",
       "23  2016-12          -0.422648\n",
       "24  2017-01          -0.376765\n",
       "25  2017-02          -0.377449\n",
       "26  2017-03          -0.374765\n",
       "27  2017-04          -0.369288\n",
       "28  2017-05          -0.377540\n",
       "29  2017-06          -0.387702\n",
       "30  2017-07          -0.375528\n",
       "31  2017-08          -0.367253\n",
       "32  2017-09          -0.360699\n",
       "33  2017-10          -0.381396\n",
       "34  2017-11          -0.392411\n",
       "35  2017-12          -0.398965\n",
       "36  2018-01          -0.386975\n",
       "37  2018-02          -0.392302\n",
       "38  2018-03          -0.398778\n",
       "39  2018-04          -0.378918\n",
       "40  2018-05          -0.342527\n",
       "41  2018-06          -0.347981\n",
       "42  2018-07          -0.386048\n",
       "43  2018-08          -0.346834\n",
       "44  2018-09          -0.392426\n",
       "45  2018-10          -0.406775\n",
       "46  2018-11          -0.409502\n",
       "47  2018-12          -0.411870\n",
       "48  2019-01          -0.375055\n",
       "49  2019-02          -0.354317\n",
       "50  2019-03          -0.363135\n",
       "51  2019-04          -0.385145\n",
       "52  2019-05          -0.416421\n",
       "53  2019-06          -0.382959\n",
       "54  2019-07          -0.387233\n",
       "55  2019-08          -0.397097\n",
       "56  2019-09          -0.399650\n",
       "57  2019-10          -0.389866\n",
       "58  2019-11          -0.377424\n",
       "59  2019-12          -0.393139"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "microsoft_tweets_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Analysis You Did - Give it a better title\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETC AD NASEUM\n",
    "\n",
    "Some more words and stuff.  Remember notebooks work best if you interleave the code that generates a result with properly annotate figures and text that puts these results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thoughtful discussion of ethical concerns included\n",
    "- Ethical concerns consider the whole data science process (question asked, data collected, data being used, the bias in data, analysis, post-analysis, etc.)\n",
    "- How your group handled bias/ethical concerns clearly described\n",
    "\n",
    "Acknowledge and address any ethics & privacy related issues of your question(s), proposed dataset(s), and/or analyses. Use the information provided in lecture to guide your group discussion and thinking. If you need further guidance, check out [Deon's Ethics Checklist](http://deon.drivendata.org/#data-science-ethics-checklist). In particular:\n",
    "\n",
    "- Are there any biases/privacy/terms of use issues with the data you propsed?\n",
    "- Are there potential biases in your dataset(s), in terms of who it composes, and how it was collected, that may be problematic in terms of it allowing for equitable analysis? (For example, does your data exclude particular populations, or is it likely to reflect particular human biases in a way that could be a problem?)\n",
    "- How will you set out to detect these specific biases before, during, and after/when communicating your analysis?\n",
    "- Are there any other issues related to your topic area, data, and/or analyses that are potentially problematic in terms of data privacy and equitable impact?\n",
    "- How will you handle issues you identified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discusison and Conclusion\n",
    "\n",
    "Wrap it all up here.  Somewhere between 3 and 10 paragraphs roughly.  A good time to refer back to your Background section and review how this work extended the previous stuff. \n",
    "\n",
    "\n",
    "# Team Contributions\n",
    "\n",
    "Speficy who did what.  This should be pretty granular, perhaps bullet points, no more than a few sentences per person."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
